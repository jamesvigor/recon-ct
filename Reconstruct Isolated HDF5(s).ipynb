{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script runs an iterative reconstruction of an in-situ hdf file or series of hdf files from the 2-BM instrument. Implementation here is designed for in-situ tests, reconstruction of ex-situ data takes a slightly different form. The pixel size must be changed from 0.875 um to 1.6 or 2.0 um. This can be optically verified in imagej or similar using the circle select tool and manually calculating it. <br>\n",
    "\n",
    "The reconstruction must be chunked for RAM allocation, the `chunk_size` variable should be set correctly for this; `chunk_size` should be set to reflect the available RAM on the system. Running this without sufficient RAM will cause execution to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/opt/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tomopy \n",
    "import tomocuda\n",
    "import os\n",
    "import numpy as np\n",
    "from tomopy.recon.rotation import write_center\n",
    "from tomopy.recon.rotation import find_center_vo\n",
    "from tomopy.recon.algorithm import recon\n",
    "from scipy import misc\n",
    "import dxchange as tir\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some directory information. We need the directory, the input file and the flat and open beam files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "total_num_slices = 2000\n",
    "chunk_size = 200\n",
    "if chunk_size > total_num_slices:\n",
    "    chunk_size = total_num_slices\n",
    "\n",
    "margin_slices = 30\n",
    "num_chunk = np.int(total_num_slices/chunk_size) + 1\n",
    "if total_num_slices == chunk_size:\n",
    "    num_chunk = 1\n",
    "       \n",
    "z = 10\n",
    "eng = 27\n",
    "pxl = 0.875\n",
    "\n",
    "zinger_level = 200\n",
    "\n",
    "data_dir  = '/run/media/james/37dd3227-85bf-4d9f-9575-ed621dc7c33b/raw/31_1day_7.5x_edge_75mm_1DegPerSec_180Deg_100msecExpTime_1500proj_Global_25umLuAG_1mmC_2mmGlass_25keV_2.657mrad_AHutch/'\n",
    "file      = 'proj_200.hdf'\n",
    "file_flat = 'proj_200.hdf'\n",
    "file_dark = 'proj_200.hdf'\n",
    "\n",
    "output_dir = data_dir\n",
    "file_name = os.path.join(data_dir, file)\n",
    "flat_name = os.path.join(data_dir, file_flat)\n",
    "dark_name = os.path.join(data_dir, file_dark)\n",
    "output_file = output_dir+'/recon_'+file.split(\".\")[-2]+'/recon_'+file.split(\".\")[-2] +'_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from the filesystem. The data are sliced here, we will work with a smaller data volume to get the centre of rotation of the specimen; this makes the operation faster as a reconstruction of the full stack is not required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = tir.read_hdf5(file_name, '/exchange/data',       slc=((1,1500), (0,2560,1)))\n",
    "white = tir.read_hdf5(flat_name, '/exchange/data_dark',  slc=((1,9),    (0,2560,1)))\n",
    "dark  = tir.read_hdf5(dark_name, '/exchange/data_white', slc=((1,9),    (0,2560,1)))\n",
    "\n",
    "data_size = data.shape\n",
    "theta = np.linspace(0,np.pi,num=data_size[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data and open beam acquisitions, remove outliers from these. We do not remove outliers from the dark current results as these are inherently outliers. This is all done using the NVIDIA CUDA `tomocuda` toolkit. Placing load on the GPU makes the operation less CPU intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = tomopy.remove_outlier_cuda(data,zinger_level,size=15)\n",
    "white = tomopy.remove_outlier_cuda(white,zinger_level,size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tomopy.prep.normalize.normalize(data,white,dark)\n",
    "#data = tomopy.prep.normalize.normalize_bg(data,air=10)\n",
    "#data[0,:,:] = data[1,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripes are removed using the Fourier wavelet method. These are quick operations and are done by the CPU,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tomopy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2701c5c2a09a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtomopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstripe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stripe_fw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sym16'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tomopy' is not defined"
     ]
    }
   ],
   "source": [
    "data = tomopy.prep.stripe.remove_stripe_fw(data,level=6,wname='sym16',sigma=2,pad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase contrast is extracted and merged with the data using the ratio defined by ``rat``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = 0.9\n",
    "data = tomopy.prep.phase.retrieve_phase(data,pixel_size=pxl,dist=z,energy=eng,alpha=rat,pad=True)\n",
    "data_size = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centre of rotation is estimated as the mid-point of the image and adjusted relative to this. An isolated slice is reconstructed with a varying centre of rotation. The optimum centre is then extracted optically from the stack. This only needs to actually be run the first time around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = data_size[2]/2-5\n",
    "write_center(data[:,19:21,:], theta, dpath='/home/james/Data/Tomography/CoR/', cen_range=(2000, 2500, 10))\n",
    "#write_center(data[:,9:11,:], theta, dpath='/home/james/Data/Tomography/CoR/', cen_range=(cs,cs+10,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(CR, time, infile, inflat, indark):\n",
    "    center = CR\n",
    "    \n",
    "    out = output_file + str(time)\n",
    "    for ii in range(num_chunk):\n",
    "        if ii == 0:\n",
    "            SliceStart = offset + ii     * chunk_size\n",
    "            SliceEnd   = offset + (ii+1) * chunk_size\n",
    "    \n",
    "        else:\n",
    "            SliceStart = offset + ii*(chunk_size-margin_slices)\n",
    "            SliceEnd = offset + SliceStart + chunk_size\n",
    "            if SliceEnd > (offset+total_num_slices):\n",
    "                SliceEnd = offset+total_num_slices\n",
    "        \n",
    "        data_dir  = '/run/media/james/37dd3227-85bf-4d9f-9575-ed621dc7c33b/raw/PCBFS_91_longscan1_10x_dimax_75mm_36DegPerSec_180Deg_2msecExpTime_600proj_Rolling_100umLuAG_1mmC_2mmGlass_pink_2.657mrad_AHutch/'\n",
    "        file_name = os.path.join(data_dir, infile)\n",
    "        flat_name = os.path.join(data_dir, inflat)\n",
    "        dark_name = os.path.join(data_dir, indark)\n",
    "        \n",
    "        data  = tir.read_hdf5(file_name,'/exchange/data_dark', slc=((1,600), (SliceStart,SliceEnd,1)))\n",
    "        white = tir.read_hdf5(flat_name,'/exchange/data_dark', slc=((1,9),   (SliceStart,SliceEnd,1)))\n",
    "        dark  = tir.read_hdf5(dark_name,'/exchange/data_dark', slc=((1,9),   (SliceStart,SliceEnd,1)))\n",
    "        data_size = data.shape\n",
    "        theta = np.linspace(0,np.pi,num=data_size[0])    \n",
    "        \n",
    "        # Damaged and/or corrupted images are replaced if \n",
    "        data[0,:,:] = data[1,:,:]\n",
    "        \n",
    "        # remove zingers (pixels with abnormal counts)\n",
    "        data  = tomocuda.remove_outlier_cuda(data,  zinger_level, size=15)\n",
    "        white = tomocuda.remove_outlier_cuda(white, zinger_level, size=15)\n",
    "        \n",
    "        # normalize projection images; for now you need to do below two operations in sequence\n",
    "        data = tomopy.prep.normalize.normalize(data,white,dark)\n",
    "        data = tomopy.prep.normalize.normalize_bg(data,air=10)\n",
    "        \n",
    "        # remove stripes in sinograms\n",
    "        data = tomopy.prep.stripe.remove_stripe_fw(data,level=8,wname='sym16',sigma=1,pad=True)\n",
    "        \n",
    "        # phase retrieval\n",
    "        data = tomopy.prep.phase.retrieve_phase(data,pixel_size=pxl,dist=z,energy=eng,alpha=rat,pad=True)\n",
    "    \n",
    "        # tomo reconstruction\n",
    "        data_recon = recon(data, theta, center=center, algorithm='gridrec')\n",
    "        \n",
    "        # save reconstructions\n",
    "        tir.writer.write_tiff_stack(data_recon[np.int(margin_slices/2):(SliceEnd-SliceStart-np.int(margin_slices/2)),:,:], \n",
    "                                                     axis = 0,\n",
    "                                                     fname = out, \n",
    "                                                     start = SliceStart+np.int(margin_slices/2),\n",
    "                                                     overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#times = [600,    1800,    3600,    5400,    7200,    21600,   28800,   43200]\n",
    "ROT   = [1041.20]\n",
    "#FILE  = ['proj_347.hdf', 'proj_356.hdf', 'proj_362.hdf', 'proj_371.hdf', 'proj_380.hdf', 'proj_452.hdf', 'proj_485.hdf','proj_560.hdf']\n",
    "#FLAT  = ['proj_348.hdf', 'proj_357.hdf', 'proj_363.hdf', 'proj_372.hdf', 'proj_381.hdf', 'proj_453.hdf', 'proj_486.hdf','proj_561.hdf']\n",
    "#DARK  = ['proj_349.hdf', 'proj_358.hdf', 'proj_364.hdf', 'proj_373.hdf', 'proj_382.hdf', 'proj_454.hdf', 'proj_487.hdf','proj_562.hdf']\n",
    "\n",
    "times = [14400]\n",
    "FILE  = ['proj_452.hdf']\n",
    "FLAT  = ['proj_453.hdf']\n",
    "DARK  = ['proj_454.hdf']\n",
    "\n",
    "print(\"Total Iterations: \" + str(len(ROT)))\n",
    "for ii in range(1):#, #len(times)):\n",
    "    print(\"Iteration \" + str(ii + 1) + \" Started... \" + str(datetime.datetime.now()))\n",
    "    reconstruct(ROT[0], times[ii], FILE[ii], FLAT[ii], DARK[ii])\n",
    "    print(\"Iteration \" + str(ii + 1) + \" Ended... \"   + str(datetime.datetime.now()))\n",
    "    \n",
    "print(\"Ended... \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquisition time at t0 20:03:09 <br>\n",
    "Acquisition time at t1 20:36:56 <br>\n",
    "Acquisition time at t2 20:59:26 <br>\n",
    "Acquisition time at t3 21:33:10 <br>\n",
    "Acquisition time at t4 22:06:54 <br>\n",
    "Acquisition time at t5 23:59:26 <br>\n",
    "Acquisition time at t6 04:50:54 <br>\n",
    "Acquisition time at t7 09:21:19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
